{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min value of the fare_amount is less than zero and min passenger count is zero. We should discard those values.\n",
    "def normalize_fare_passenger(df):\n",
    "    if 'fare_amount' in df.columns:\n",
    "        print(\"old lenght: %d\" %len(df))\n",
    "        df = df[df.fare_amount>0]\n",
    "    print(\"length after fare_amount normalization: %d\" %len(df))\n",
    "    df = df[df.passenger_count>0]\n",
    "    print(\"length after passenger_count normalization: %d\" %len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df):\n",
    "    test_time = df['pickup_datetime'].astype(str).str[:-4]\n",
    "    df['date_time'] =  pd.to_datetime(test_time, format='%Y%m%d %H:%M:%S')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(data):\n",
    "    data['hour'] = data['date_time'].dt.hour\n",
    "    data['day'] = data['date_time'].dt.day\n",
    "    data['month'] = data['date_time'].dt.month\n",
    "    data['year'] = data['date_time'].dt.year\n",
    "    data['weekday'] = data['date_time'].dt.weekday\n",
    "    data = data.drop(['date_time','pickup_datetime'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define haversine distance\n",
    "def dist(pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude):\n",
    "    pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude = map(np.radians, [pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude])\n",
    "    dlon = dropoff_longitude - pickup_longitude\n",
    "    dlat = dropoff_latitude - pickup_latitude\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(pickup_latitude) * np.cos(dropoff_latitude) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    distance = 6367 * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances to nearby city center, airports, and other ny counties\n",
    "def transform(data):\n",
    "    # Distances to nearby airports, city center and other counties\n",
    "    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n",
    "    \n",
    "    # city center\n",
    "    nyc = (-74.0060, 40.7128)\n",
    "    \n",
    "    # county\n",
    "    Nassau = (-73.5594, 40.6546)\n",
    "    Suffolk = (-72.6151, 40.9849)\n",
    "    Westchester = (-73.7949, 41.1220)\n",
    "    Rockland = (-73.9830, 41.1489)\n",
    "    Dutchess = (-73.7478, 41.7784)\n",
    "    Orange = (-74.3118, 41.3912)\n",
    "    Putnam = (-73.7949, 41.4351) \n",
    "\n",
    "    # airport\n",
    "    jfk = (-73.7781, 40.6413)\n",
    "    ewr = (-74.1745, 40.6895)\n",
    "    lgr = (-73.8740, 40.7769)\n",
    "    \n",
    "    \n",
    "    # county\n",
    "    data['pickup_distance_to_center'] = dist(nyc[0], nyc[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_center'] = dist(nyc[0], nyc[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_Nassau'] = dist(Nassau[0], Nassau[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_Nassau'] = dist(Nassau[0], Nassau[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_Suffolk'] = dist(Suffolk[0], Suffolk[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_Suffolk'] = dist(Suffolk[0], Suffolk[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_Westchester'] = dist(Westchester[0], Westchester[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_Westchester'] = dist(Westchester[0], Westchester[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_Rockland'] = dist(Rockland[0], Rockland[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_Rockland'] = dist(Rockland[0], Rockland[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_Dutchess'] = dist(Dutchess[0], Dutchess[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_Dutchess'] = dist(Dutchess[0], Dutchess[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_Orange'] = dist(Orange[0], Orange[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_Orange'] = dist(Orange[0], Orange[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_Putnam'] = dist(Putnam[0], Putnam[1],\n",
    "                                      data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_Putnam'] = dist(Putnam[0], Putnam[1],\n",
    "                                      data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    # airports\n",
    "    data['pickup_distance_to_jfk'] = dist(jfk[0], jfk[1],\n",
    "                                         data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_jfk'] = dist(jfk[0], jfk[1],\n",
    "                                           data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_ewr'] = dist(ewr[0], ewr[1], \n",
    "                                          data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_ewr'] = dist(ewr[0], ewr[1],\n",
    "                                           data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    data['pickup_distance_to_lgr'] = dist(lgr[0], lgr[1],\n",
    "                                          data['pickup_longitude'], data['pickup_latitude'])\n",
    "    data['dropoff_distance_to_lgr'] = dist(lgr[0], lgr[1],\n",
    "                                           data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    # point distance\n",
    "    data['distance'] = dist(data['pickup_longitude'], data['pickup_latitude'],\n",
    "                            data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_convert(df):\n",
    "\n",
    "    # There is a 50-cent MTA State Surcharge for all trips that end in New York City or \n",
    "    # Nassau, Suffolk, Westchester, Rockland, Dutchess, Orange or Putnam Counties.\n",
    "    # The following two variables can be merged into one.\n",
    "    # The following only considers trips that starts in city center and ends in nearby counties,\n",
    "    # while the opposite direction could also be considered\n",
    "    # counties\n",
    "    df['county_dropoff_1'] = np.where((df['pickup_distance_to_center'] <= 5) &\n",
    "                                     ((df['dropoff_distance_to_Nassau'] <= 21.3) |\n",
    "                                      (df['dropoff_distance_to_Westchester'] <= 22.4)), 1, 0)\n",
    "    \n",
    "    df['county_dropoff_2'] = np.where((df['pickup_distance_to_center'] <= 5) &                  \n",
    "                                     ((df['dropoff_distance_to_Suffolk'] <= 48.7) |           \n",
    "                                      (df['dropoff_distance_to_Rockland'] <= 14.1) |\n",
    "                                      (df['dropoff_distance_to_Dutchess'] <= 28.7) |\n",
    "                                      (df['dropoff_distance_to_Orange'] <= 29) |\n",
    "                                      (df['dropoff_distance_to_Putnam'] <= 15.7)), 1, 0)\n",
    "    \n",
    "    # There is a daily 50-cent surcharge from 8pm to 6am.\n",
    "    df['night_hour'] = np.where((df['hour'] >= 20) |\n",
    "                                (df['hour'] <= 6) , 1, 0)\n",
    "    \n",
    "    # There is a $1 surcharge from 4pm to 8pm on weekdays, excluding holidays.\n",
    "    df['peak_hour'] = np.where((df['hour'] >= 16) &\n",
    "                                (df['hour'] <= 20) & \n",
    "                                (df['weekday'] >=0) &\n",
    "                                (df['weekday'] <=4) , 1, 0)\n",
    "    \n",
    "    # This is a flat fare of $52 plus tolls, the 50-cent MTA State Surcharge, the 30-cent Improvement Surcharge, \n",
    "    # to/from JFK and any location in Manhattan:\n",
    "    df['to_from_jfk'] = np.where(((df['pickup_distance_to_jfk'] <= 2) & (df['dropoff_distance_to_center'] <= 5)) | \n",
    "                                 ((df['pickup_distance_to_center'] <= 5) & (df['dropoff_distance_to_jfk'] <= 2)) ,1, 0)\n",
    "\n",
    "    # There is a $4.50 rush hour surcharge (4 PM to 8 PM weekdays, excluding legal holidays). o/from JFK and any location in Manhattan:\n",
    "    df['jfk_rush_hour'] = np.where((df['to_from_jfk'] == 1) & \n",
    "                                   (df['hour'] >= 16) &\n",
    "                                   (df['hour'] <= 20) ,1, 0)\n",
    "    \n",
    "    # There is a $17.50 Newark Surcharge to Newark Airport:\n",
    "    df['ewr'] = np.where((df['pickup_distance_to_center'] <= 5) &\n",
    "                         (df['dropoff_distance_to_ewr'] <= 1) ,1, 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_COEFFICIENT = 3\n",
    "def outlier_analysis(df):\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        std = df[column].std()\n",
    "        mean = df[column].mean()\n",
    "        non_outlier_df = df[(df[column] > mean - STD_COEFFICIENT*std) & (df[column] < mean + STD_COEFFICIENT*std)]\n",
    "        if len(non_outlier_df) > 0:\n",
    "            df = non_outlier_df\n",
    "        else:\n",
    "            print(\"column %s  : all elements outlier\"%column)\n",
    "        print(\"column %s applied\"%column)\n",
    "        print(\"new lenght: %d\" %len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histogram(df, column_name, color, edge_color):\n",
    "    fig, ax = plt.subplots()\n",
    "    df[column_name].hist(color=color, edgecolor=edge_color,  \n",
    "                              grid=False)\n",
    "    ax.set_title(column_name + ' Histogram', fontsize=12)\n",
    "    ax.set_xlabel(column_name, fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bin_labels(df, column, quantile_list, quantile_labels):\n",
    "    quantile_list_name = column + '_bin_custom_range'\n",
    "    quantile_labels_name = column + '_bin_custom_label'\n",
    "    df[quantile_list_name] = pd.qcut(\n",
    "                                            df[column], \n",
    "                                            q=quantile_list)\n",
    "    df[quantile_labels_name] = pd.qcut(\n",
    "                                                df[column], \n",
    "                                                q=quantile_list,       \n",
    "                                                labels=quantile_labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequential_model(X_train, X_test, y_train, y_test, batch_size, epochs, learning_rate, verbose):\n",
    "    \n",
    "    input_size = X_train.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, activation='relu', input_shape=(input_size,), activity_regularizer=regularizers.l1(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mse'])\n",
    "    \n",
    "    train_history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_data=(X_test,y_test), \n",
    "                    shuffle=True)\n",
    "    \n",
    "    return train_history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_weights(model, model_json_name, model_weight_name):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_weight_name)\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_weights(model, model_json_name, model_weight_name):\n",
    "    # load json and create model\n",
    "    json_file = open(model_json_name, \"r\")\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_weight_name)\n",
    "    print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_build(train_history):\n",
    "    # plotting train_history\n",
    "    plt.figure(0)\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,90])\n",
    "    plt.plot(train_history.history['loss'],'g')\n",
    "    plt.plot(train_history.history['val_loss'],'b')\n",
    "    plt.rcParams['figure.figsize'] = (8, 6) \n",
    "    plt.xlabel(\"Num of Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss vs Validation Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend(['train','validation'])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_build_train_val_ratio(train_history):\n",
    "    # plotting train_history\n",
    "    plt.figure(0)\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,5])\n",
    "    plt.plot([x/y for x, y in zip(train_history.history['loss'], train_history.history['val_loss'])],'g')\n",
    "    plt.rcParams['figure.figsize'] = (8, 6) \n",
    "    plt.xlabel(\"Num of Epochs\")\n",
    "    plt.ylabel(\"Loss / Val_loss\")\n",
    "    plt.title(\"Training Loss and Validation Loss Ratio\")\n",
    "    plt.grid()\n",
    "    plt.legend(['ratio'])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_build_mse(train_history):\n",
    "    # plotting train_history\n",
    "    plt.figure(0)\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,100])\n",
    "    plt.plot(train_history.history['mean_squared_error'],'g')\n",
    "    plt.plot(train_history.history['val_mean_squared_error'],'r')\n",
    "    plt.rcParams['figure.figsize'] = (8, 6) \n",
    "    plt.xlabel(\"Num of Epochs\")\n",
    "    plt.ylabel(\"mse\")\n",
    "    plt.title(\"MSE\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss_mse', 'val_mse'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
